{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOpSnTw5eo3GWJxghlH/jB7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ValerianFourel/DSReal/blob/main/NeuralNetwork/NetworkElevation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Q_oA1s3XECBw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07f6d1d0-aa18-4edf-f713-85f9a31f2500"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import dask.dataframe as dd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "import random"
      ],
      "metadata": {
        "id": "tHOVg8yPe3xL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Replace the zip_file_path with the path to the zip file in your Google Drive\n",
        "zip_file_pathElevation = '/content/drive/MyDrive/dataElevationTensor.zip'\n",
        "\n",
        "zip_file_paths = [zip_file_pathElevation]\n",
        "\n",
        "# Replace the destination_folder with the path of the folder where you want to extract the contents\n",
        "destination_folderElevation = '/content/dataElevation'\n",
        "destination_folders = [destination_folderElevation]"
      ],
      "metadata": {
        "id": "QJPWVQyfIY9i"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the file\n",
        "\n",
        "for i in range(len(destination_folders)):\n",
        "  with zipfile.ZipFile(zip_file_paths[i], 'r') as zip_ref:\n",
        "      zip_ref.extractall(destination_folders[i])\n"
      ],
      "metadata": {
        "id": "6DfqQr97e7eB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampleCoordinatesElevationIDArrayPositionDf_file = '/content/drive/MyDrive/sampleCoordinatesElevationIDArrayPositionDf.parquet'\n",
        "sampleCoordinatesElevationIDArrayPositionDf = dd.read_parquet(sampleCoordinatesElevationIDArrayPositionDf_file).compute()\n"
      ],
      "metadata": {
        "id": "bKchvQGHe-Yw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2 Custom Dataset"
      ],
      "metadata": {
        "id": "BXIHPetJIyYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CustomRasterDataset(torch.utils.data.Dataset):\n",
        "    'Characterizes a dataset for PyTorch'\n",
        "    def __init__(self, dataFrame, file_path, file_extension,windowSize,re_scale = False,new_min = -1,new_max = 1):\n",
        "        'Initialization'\n",
        "        self.re_scale = re_scale\n",
        "        self.new_min  = new_min\n",
        "        self.new_max = new_max\n",
        "        self.dataFrame = dataFrame\n",
        "        self.file_path = file_path\n",
        "        self.file_extension = file_extension\n",
        "        self.windowSize = windowSize\n",
        "        self.offset = self.windowSize // 2\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.dataFrame)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        ID =  self.dataFrame.iloc[index]['ID'] # str(ID).rstrip('.0')\n",
        "\n",
        "        x = self.dataFrame.iloc[index]['x'] + random.choice([-2, -1, 0, 1, 2])\n",
        "        y = self.dataFrame.iloc[index]['y'] + random.choice([-2, -1, 0, 1, 2])\n",
        "\n",
        "        # Load data and get label\n",
        "        fullArray = torch.load(self.file_path+ID+self.file_extension)\n",
        "        # Determine the window for the square\n",
        "        left = x - self.offset\n",
        "        right = x + (self.offset + 1)\n",
        "        top = y - self.offset\n",
        "        bottom = y + (self.offset + 1)\n",
        "        X = fullArray[left:right,top:bottom].clone().detach()  # Access value in gpu_dictElevation\n",
        "        # if self.re_scale:\n",
        "          # X =  re_scale(X,self.new_min,self.new_max)\n",
        "        return X.unsqueeze(0)"
      ],
      "metadata": {
        "id": "Px8WOkYyIseD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_pathElevation = '/content/dataElevation/'\n"
      ],
      "metadata": {
        "id": "Dn8wHAhWfXvg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_sizeElevation = 32"
      ],
      "metadata": {
        "id": "znAmNUczfbtE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_extension = '.pt'\n",
        "num_workers = 2"
      ],
      "metadata": {
        "id": "aSnUlKfUfZqH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "windowSizeElevation = 65\n",
        "\n",
        "# Create the dataset instance\n",
        "datasetElevation = CustomRasterDataset(sampleCoordinatesElevationIDArrayPositionDf, file_pathElevation, file_extension,windowSizeElevation)"
      ],
      "metadata": {
        "id": "dzFqPJUnfNbp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a DataLoader for batching and parallel data loading (you can adjust batch_size and num_workers as needed)\n",
        "\n",
        "dataLoaderElevation = DataLoader(datasetElevation, batch_size=batch_sizeElevation, num_workers=num_workers, shuffle=True)\n",
        "\n",
        "# Now you can use dataLoaderEvapo in your training loop to efficiently access the elevation values in gpu_dictElevation.\n"
      ],
      "metadata": {
        "id": "cJOHungAfSHM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have already created the 'elevation_dataloader' as mentioned in the previous steps\n",
        "\n",
        "# Get the first batch from the dataloader using the 'next' function\n",
        "first_batch = next(iter(dataLoaderElevation))\n",
        "second_batch = next(iter(dataLoaderElevation))\n",
        "# Print the content of the first batch\n",
        "print(\"First Batch:\")\n",
        "print(first_batch.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9CP_y_bfhrq",
        "outputId": "ca788577-53d0-47ef-be38-33af66132088"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Batch:\n",
            "torch.Size([32, 1, 65, 65])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3. Architecture of the Elevation VAE"
      ],
      "metadata": {
        "id": "LwZFg-xTNGdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "YoOItC8jfqhW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model: nn.Module) -> int:\n",
        "    \"\"\"\n",
        "    Count the number of parameters in a PyTorch model.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): PyTorch model.\n",
        "\n",
        "    Returns:\n",
        "        int: Total number of parameters.\n",
        "    \"\"\"\n",
        "    return sum(p.numel() for p in model.parameters())\n"
      ],
      "metadata": {
        "id": "_i2IcfNbDntl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ResDown(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual down sampling block for the encoder\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channel_in, channel_out, kernel_size=3):\n",
        "        super(ResDown, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channel_in, channel_out // 2, kernel_size, 2, kernel_size // 2)\n",
        "        self.bn1 = nn.BatchNorm2d(channel_out // 2, eps=1e-4)\n",
        "        self.conv2 = nn.Conv2d(channel_out // 2, channel_out, kernel_size, 1, kernel_size // 2)\n",
        "        self.bn2 = nn.BatchNorm2d(channel_out, eps=1e-4)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(channel_in, channel_out, kernel_size, 2, kernel_size // 2)\n",
        "\n",
        "        self.act_fnc = nn.ELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip = self.conv3(x)\n",
        "        x = self.act_fnc(self.bn1(self.conv1(x)))\n",
        "        x = self.conv2(x)\n",
        "        return self.act_fnc(self.bn2(x + skip))\n",
        "\n",
        "\n",
        "class ResUp(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual up sampling block for the decoder\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channel_in, channel_out, kernel_size=3, scale_factor=2):\n",
        "        super(ResUp, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(channel_in, channel_in // 2, kernel_size, 1, kernel_size // 2)\n",
        "        self.bn1 = nn.BatchNorm2d(channel_in // 2, eps=1e-4)\n",
        "        self.conv2 = nn.Conv2d(channel_in // 2, channel_out, kernel_size, 1, kernel_size // 2)\n",
        "        self.bn2 = nn.BatchNorm2d(channel_out, eps=1e-4)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(channel_in, channel_out, kernel_size, 1, kernel_size // 2)\n",
        "\n",
        "        self.up_nn = nn.Upsample(scale_factor=scale_factor, mode=\"nearest\")\n",
        "\n",
        "        self.act_fnc = nn.ELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up_nn(x)\n",
        "        skip = self.conv3(x)\n",
        "        x = self.act_fnc(self.bn1(self.conv1(x)))\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        return self.act_fnc(self.bn2(x + skip))\n"
      ],
      "metadata": {
        "id": "pA6hp4syNKK7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the encoder network\n",
        "class ElevationEncoder(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(ElevationEncoder, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.conv_in = nn.Conv2d(1, 4, 7, 1, 3)\n",
        "        self.res_down_block1 = ResDown(4, 8)\n",
        "        self.res_down_block2 = ResDown(8, 16)\n",
        "        self.res_down_block3 = ResDown(16,32)\n",
        "        self.res_down_block4 = ResDown(32, 64)\n",
        "        self.conv_mu = nn.Conv2d(32, latent_dim, 9, 1)\n",
        "        self.conv_log_var = nn.Conv2d(32, latent_dim, 9, 1)\n",
        "        self.act_fnc = nn.ELU()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act_fnc(self.conv_in(x))\n",
        "        x = self.res_down_block1(x)  # 32\n",
        "        x = self.res_down_block2(x)  # 16\n",
        "        x = self.res_down_block3(x)  # 8\n",
        "        mu = self.conv_mu(x)  # 1\n",
        "        logvar = self.conv_log_var(x)  # 1\n",
        "\n",
        "        return mu, logvar\n",
        "\n",
        "# Define the decoder network\n",
        "class ElevationDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(ElevationDecoder, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.conv_t_up = nn.ConvTranspose2d(latent_dim, 64, 4, 1)\n",
        "        self.res_up_block1 = ResUp(64, 32)\n",
        "        self.res_up_block2 = ResUp(32, 16)\n",
        "        self.res_up_block3 = ResUp(16,8)\n",
        "        self.res_up_block4 = ResUp(8,8)\n",
        "\n",
        "        self.conv_out1 = nn.Conv2d(8, 1, 4, stride=1, padding=2)\n",
        "        # self.conv_out2 = nn.Conv2d(4, 1, 4, stride=1, padding=2)\n",
        "        # self.conv_out3 = nn.Conv2d(2, 1, 3, 1, 1)\n",
        "\n",
        "        self.act_fnc = nn.ELU()\n",
        "        self.act_fnc2 = nn.ELU()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0], self.latent_dim, 1, 1)\n",
        "        x = self.act_fnc(self.conv_t_up(x))  # 4\n",
        "        x = self.res_up_block1(x)  # 8\n",
        "        x = self.res_up_block2(x)  # 16\n",
        "        x = self.res_up_block3(x)  # 32\n",
        "        x = self.res_up_block4(x)  # 32\n",
        "\n",
        "        # x = self.act_fnc2(self.conv_out1(x))\n",
        "        x = self.conv_out1(x)\n",
        "        # x = torch.tanh(self.conv_out3(x))\n",
        "        return x\n",
        "\n",
        "# Combine the encoder and decoder to form the VAE\n",
        "class ElevationVAE(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(ElevationVAE, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.encoder = ElevationEncoder(latent_dim)\n",
        "        self.decoder = ElevationDecoder(latent_dim)\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = mu + eps * std\n",
        "        return z\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        reconstructed_x = self.decode(z)\n",
        "        return reconstructed_x, mu, logvar\n",
        "\n",
        "\n",
        "# Instantiate the VAE with the desired latent_dim\n",
        "latent_dim = 25\n",
        "vae = ElevationVAE(latent_dim)\n",
        "\n",
        "# Pass the input batch through the VAE\n",
        "reconstructed_batch, mu, logvar = vae(first_batch)\n",
        "\n",
        "# Check the output shape\n",
        "print(\"Reconstructed batch shape:\", reconstructed_batch.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJ7KYTzmf1tc",
        "outputId": "d106d9c8-e131-4f79-def1-6cfec3164205"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reconstructed batch shape: torch.Size([32, 1, 65, 65])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Instantiate the VAE with the desired latent_dim\n",
        "latent_dim = 25\n",
        "vae = ElevationVAE(latent_dim)\n"
      ],
      "metadata": {
        "id": "FYWJ6IzBp8Av"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-CLPW5G_DrNt"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass the input batch through the VAE\n",
        "reconstructed_batch, mu, logvar = vae(first_batch)\n",
        "\n",
        "# Check the output shape\n",
        "print(\"Reconstructed batch shape:\", reconstructed_batch.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Z1fCuQFp9pm",
        "outputId": "1657eda2-d09e-40a6-8247-35d35a10be1f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reconstructed batch shape: torch.Size([32, 1, 65, 65])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_parameters(vae)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDmWnQ35Dr2L",
        "outputId": "97a4a219-11bf-4939-a997-3e464d363e1a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "279571"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part4 Training the VAE"
      ],
      "metadata": {
        "id": "86YB0R2yhv1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_weight = len(sampleCoordinatesElevationIDArrayPositionDf)/batch_sizeElevation\n",
        "loss_weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtyPIlHvDFb8",
        "outputId": "a509e4f3-d83e-4220-8c92-bb00d6b0146c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29555.59375"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "kJWeF2zChx8x"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "vPnWFkvhhz1k"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity"
      ],
      "metadata": {
        "id": "Z1xQHz89h1kO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lpips = LearnedPerceptualImagePatchSimilarity(net_type='squeeze')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkGzSHDfh3iT",
        "outputId": "184ac805-4b77-4838-8ffa-b504ebaf30ee"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/squeezenet1_1-b8a52dc0.pth\" to /root/.cache/torch/hub/checkpoints/squeezenet1_1-b8a52dc0.pth\n",
            "100%|██████████| 4.73M/4.73M [00:00<00:00, 51.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "F7IZGLA-h7c1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_tensor01(tensor):\n",
        "    max_val = torch.max(tensor)\n",
        "    min_val = torch.min(tensor)\n",
        "\n",
        "    # Avoid division by zero\n",
        "    if max_val - min_val != 0:\n",
        "        normalized_tensor = (tensor - min_val) / (max_val - min_val)\n",
        "    else:\n",
        "        normalized_tensor = tensor - min_val\n",
        "\n",
        "    return normalized_tensor\n",
        "\n",
        "\n",
        "def normalized_mse(tensor1, tensor2):\n",
        "    # Normalize the tensors\n",
        "    norm_tensor1 = normalize_tensor01(tensor1)\n",
        "    norm_tensor2 = normalize_tensor01(tensor2)\n",
        "\n",
        "    # Compute MSE\n",
        "    mse_loss_normalized = F.mse_loss(norm_tensor1, norm_tensor2)\n",
        "    mse_loss =  F.mse_loss(tensor1, tensor2)\n",
        "\n",
        "    return mse_loss_normalized, mse_loss"
      ],
      "metadata": {
        "id": "06r8ciXbGubA"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ElevationVAELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ElevationVAELoss, self).__init__()\n",
        "\n",
        "\n",
        "    def forward(self, reconstructed_x, x_final, x, mu, logvar,lpips):\n",
        "        # Repeat the last two dimensions three times\n",
        "        # reconstructed_x_repeated =  rescale_tensor(x_final).repeat(1, 3, 1, 1)\n",
        "        # x_repeated = rescale_tensor(x).repeat(1, 3, 1, 1)\n",
        "\n",
        "        # Compute the Mean Squared Error (MSE) reconstruction loss\n",
        "       #  lpips = lpips(reconstructed_x_repeated, x_repeated)\n",
        "        # Create the L1 loss function\n",
        "        loss_value_l1 = nn.L1Loss(reduction='mean')(reconstructed_x,x)\n",
        "        # Compute the KL divergence term\n",
        "        kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "        # Return the sum of the reconstruction loss and KL divergence term\n",
        "        return kl_divergence +  (loss_value_l1)*loss_weight"
      ],
      "metadata": {
        "id": "Exu1HNuwiAWi"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.init as init\n",
        "\n",
        "def reset_parameters(model):\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
        "            # Reset the weights and biases of Conv2d and Linear layers\n",
        "            module.reset_parameters()\n",
        "        elif isinstance(module, nn.BatchNorm2d):\n",
        "            # Reset the running statistics of BatchNorm2d layers\n",
        "            module.reset_running_stats()\n",
        "\n",
        "# Assuming you have already instantiated the VAE\n",
        "vae = ElevationVAE(latent_dim)\n",
        "\n",
        "# Reinitialize the VAE's parameters\n",
        "reset_parameters(vae)\n"
      ],
      "metadata": {
        "id": "CxHq-rqEiX96"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rateElevation = 0.0001"
      ],
      "metadata": {
        "id": "pAlXoBIfiZ6Z"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dimElevation = 10\n"
      ],
      "metadata": {
        "id": "NI6xgznnif4p"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the function to train the VAE\n",
        "def train_vae(vae, dataloader, num_epochs, learning_rate):\n",
        "    # Set the model to training mode\n",
        "    vae.train()\n",
        "\n",
        "    # Define the Mean Squared Error (MSE) loss function\n",
        "    criterion = ElevationVAELoss()\n",
        "    mse = torch.nn.MSELoss()\n",
        "\n",
        "    # Define the optimizer (you can experiment with different optimizers)\n",
        "    optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
        "    total_batches = len(dataloader)\n",
        "    batches_done = 0\n",
        "    lpips = LearnedPerceptualImagePatchSimilarity(net_type='squeeze').to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        batches_done =0\n",
        "        mse_loss = 0\n",
        "        for batch_idx, data in enumerate(dataloader):\n",
        "            # Get the batch of data and move it to the device (e.g., GPU if available)\n",
        "            # inputs = data\n",
        "            dimensions = data.shape\n",
        "\n",
        "            inputs = data.to(device)\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            reconstructed_batch, mu, logvar = vae(inputs)\n",
        "            # Compute the MSE loss\n",
        "            loss = criterion(reconstructed_batch, reconstructed_batch, inputs,mu, logvar,lpips)\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "\n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update the total loss for the epoch\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Update the number of batches processed\n",
        "            batches_done += 1\n",
        "            mse_loss += mse(normalize_tensor01(reconstructed_batch),normalize_tensor01(inputs))\n",
        "\n",
        "            # Print the progress when a tenth of the epoch is completed\n",
        "            if batches_done % (len(dataloader) // 10) == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}] - Progress: {batches_done}/{len(dataloader)} - Total Loss: {total_loss / (len(dataloader) // 10)}, {mse_loss.item() / (len(dataloader) // 10)}\")\n",
        "                total_loss = 0\n",
        "                mse_loss = 0\n",
        "\n",
        "        # Print the average loss for the epoch\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {total_loss / ((len(dataloader) - len(dataloader) //10 * 9 ) % 10)}\")\n"
      ],
      "metadata": {
        "id": "8KeLj7-zis-l"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example usage:'device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")'\n",
        "# Assuming you have the training data in 'train_dataloader' and a device set, e.g.,\n",
        "# Instantiate the VAE with the desired latent_dim\n",
        "vae = ElevationVAE(latent_dimElevation).to(device)\n",
        "\n",
        "# Define the number of epochs and learning rate\n",
        "num_epochs = 3\n"
      ],
      "metadata": {
        "id": "RVpSfFA6iwNK"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Compute average normalized MSE for the first 100 batches\n",
        "def compute_average_mse(dataloader):\n",
        "    mse_values = []\n",
        "    count = 0\n",
        "\n",
        "    for data in dataloader:\n",
        "        if count >= 10000:\n",
        "            break\n",
        "\n",
        "        # Generate random \"predictions\" just for the sake of the example\n",
        "        # In a real scenario, these would be the model's output\n",
        "        reconstructed_batch, _, _ = vae(data.to(device))\n",
        "\n",
        "        mse,mse2 = normalized_mse(data.cpu(), reconstructed_batch.cpu())\n",
        "        mse_values.append(mse.item())\n",
        "\n",
        "        count += 1\n",
        "\n",
        "    average_mse = sum(mse_values) / len(mse_values)\n",
        "    print(\"Average Normalized MSE:\", average_mse)\n",
        "\n"
      ],
      "metadata": {
        "id": "6--M7y5CEymh"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the VAE\n",
        "train_vae(vae, dataLoaderElevation, num_epochs, learning_rateElevation )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "Vp7H1p9Liuv7",
        "outputId": "012f1f58-e1d0-4d6e-9798-873cff9e6148"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/3] - Progress: 2955/29556 - Total Loss: 10226068.646531302, 1296.55810546875\n",
            "Epoch [1/3] - Progress: 5910/29556 - Total Loss: 8998230.708883248, 1134.6751708984375\n",
            "Epoch [1/3] - Progress: 8865/29556 - Total Loss: 7195097.149323181, 721.9560546875\n",
            "Epoch [1/3] - Progress: 11820/29556 - Total Loss: 5287431.784856176, 341.4059753417969\n",
            "Epoch [1/3] - Progress: 14775/29556 - Total Loss: 3256411.9639805416, 106.93756866455078\n",
            "Epoch [1/3] - Progress: 17730/29556 - Total Loss: 2262062.591751269, 35.2636604309082\n",
            "Epoch [1/3] - Progress: 20685/29556 - Total Loss: 1901283.5876269036, 18.226003646850586\n",
            "Epoch [1/3] - Progress: 23640/29556 - Total Loss: 1751561.619712352, 13.463068008422852\n",
            "Epoch [1/3] - Progress: 26595/29556 - Total Loss: 1670775.9854060914, 11.213069915771484\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-7c4aa5d20e30>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the VAE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_vae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataLoaderElevation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rateElevation\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-41-5bcb4b970cf5>\u001b[0m in \u001b[0;36mtrain_vae\u001b[0;34m(vae, dataloader, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m# Update the total loss for the epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m# Update the number of batches processed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(vae, 'vaeElevation3Epoch.pt')"
      ],
      "metadata": {
        "id": "zKsjRqU6j15L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv vaeElevation3Epoch.pt \"/content/drive/MyDrive/Colab Notebooks/\""
      ],
      "metadata": {
        "id": "gCwGS0ZCj6Qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "# Replace `your_dataloader` with the DataLoader you have\n",
        "compute_average_mse(dataLoaderElevation)"
      ],
      "metadata": {
        "id": "Abx-oJQ4E3_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_vae(vae, dataLoaderElevation, 2, learning_rateElevation )"
      ],
      "metadata": {
        "id": "mKOCBoRjEM6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(vae, 'vaeElevation5Epoch.pt')"
      ],
      "metadata": {
        "id": "OmIo6NXPEXQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv vaeElevation5Epoch.pt \"/content/drive/MyDrive/Colab Notebooks/\""
      ],
      "metadata": {
        "id": "3wGsZNwIEZZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "# Replace `your_dataloader` with the DataLoader you have\n",
        "compute_average_mse(dataLoaderElevation)"
      ],
      "metadata": {
        "id": "XKvHmIVLE5oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_vae(vae, dataLoaderElevation, 2, learning_rateElevation )"
      ],
      "metadata": {
        "id": "RiqWzreSvFNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_parameters(vae)"
      ],
      "metadata": {
        "id": "VK1wMRk2we8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 5. Plotting the results ofthe Elevation VAE"
      ],
      "metadata": {
        "id": "YJNZFRwjMO0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_random_batch(dataloader):\n",
        "    # Get the total number of batches in the DataLoader\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    # Generate a random index to select a batch\n",
        "    random_batch_index = torch.randint(0, 100, (1,))\n",
        "\n",
        "    # Iterate through the DataLoader to find the batch at the random index\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        if i == random_batch_index:\n",
        "            return batch\n"
      ],
      "metadata": {
        "id": "SXwUOTKYMSCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "randomBatch = get_random_batch(dataLoaderElevation)"
      ],
      "metadata": {
        "id": "1MRFYys1vyDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def compare_vae_reconstruction(vae, data_batch, device):\n",
        "    # Set the VAE to evaluation mode\n",
        "    vae.eval()\n",
        "\n",
        "    # Get the batch size and number of channels\n",
        "    batch_size, num_channels, height, width = data_batch.size()\n",
        "\n",
        "    # Get the reconstructed images from the VAE\n",
        "    with torch.no_grad():\n",
        "        dimensions = data_batch.shape\n",
        "        data_batch = data_batch\n",
        "        print(dimensions)\n",
        "\n",
        "        reconstructed_batch, _, _ = vae(data_batch.to(device))\n",
        "    # Convert the tensors to numpy arrays and transpose the dimensions\n",
        "    original_images = data_batch.cpu().numpy().transpose(0,2,3,1)\n",
        "    print(reconstructed_batch.shape)\n",
        "    reconstructed_images = reconstructed_batch.cpu().numpy().transpose(0,2,3,1)\n",
        "    print(reconstructed_images.shape)\n",
        "\n",
        "\n",
        "    # Plot the original and reconstructed images side by side\n",
        "    plt.figure(figsize=(100, 100))\n",
        "    for i in range(batch_size):\n",
        "        plt.subplot(batch_size, 16, i*16 + 1)\n",
        "        plt.imshow(original_images[i])\n",
        "        plt.axis('off')\n",
        "        plt.title('Original')\n",
        "\n",
        "        plt.subplot(batch_size, 16, i*16 + 2)\n",
        "        plt.imshow(reconstructed_images[i])\n",
        "        plt.axis('off')\n",
        "        plt.title('Reconstructed')\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Usage\n",
        "# Assuming 'vae' is your trained Variational Autoencoder model\n",
        "# and 'data_batch' is your batch of input images\n",
        "# 'device' should be the device on which your model is (e.g., 'cuda' or 'cpu')\n",
        "compare_vae_reconstruction(vae, randomBatch, device)\n"
      ],
      "metadata": {
        "id": "esrxPcRpv07V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}